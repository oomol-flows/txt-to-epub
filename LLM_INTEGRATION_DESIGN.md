# TXTè½¬EPUBå¤§æ¨¡å‹æ™ºèƒ½åŒ–è®¾è®¡æ–¹æ¡ˆ

## ğŸ“– ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [å®æ–½æŒ‡å—](#å®æ–½æŒ‡å—)
- [æˆæœ¬ä¼˜åŒ–](#æˆæœ¬ä¼˜åŒ–)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
- [å®æ–½è·¯çº¿å›¾](#å®æ–½è·¯çº¿å›¾)

---

## æ¦‚è¿°

### è®¾è®¡ç›®æ ‡

å°†å¤§è¯­è¨€æ¨¡å‹(LLM)é›†æˆåˆ°ç°æœ‰çš„TXTåˆ°EPUBè½¬æ¢ç³»ç»Ÿä¸­,è§£å†³ä»¥ä¸‹é—®é¢˜:

1. **æå‡å‡†ç¡®ç‡**: ä»82%æå‡åˆ°95%+
2. **å¤„ç†ç‰¹æ®Šæ ¼å¼**: æ”¯æŒéæ ‡å‡†ç« èŠ‚æ ‡è®°
3. **æ™ºèƒ½æ¶ˆæ­§**: åŒºåˆ†ç« èŠ‚æ ‡é¢˜ä¸æ­£æ–‡å¼•ç”¨
4. **è‡ªé€‚åº”èƒ½åŠ›**: é€‚åº”å„ç§ä¹¦ç±æ ¼å¼

### æ ¸å¿ƒç†å¿µ

**æ··åˆç­–ç•¥**: è§„åˆ™ä¼˜å…ˆ + LLMè¾…åŠ©

- âœ… è§„åˆ™å¤„ç†90%æ ‡å‡†æƒ…å†µ(å¿«é€Ÿã€å…è´¹)
- âœ… LLMå¤„ç†10%å›°éš¾æƒ…å†µ(å‡†ç¡®ã€æ™ºèƒ½)
- âœ… æˆæœ¬æ§åˆ¶: ~$0.01-0.05/æœ¬ä¹¦

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¾“å…¥: TXTæ–‡æœ¬                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç¬¬ä¸€é˜¶æ®µ: ä¼ ç»Ÿè§„åˆ™è§£æ (å¿«é€Ÿ)                  â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ â€¢ æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…                          â”‚          â”‚
â”‚  â”‚ â€¢ ç« èŠ‚è¾¹ç•Œè¯†åˆ«                            â”‚          â”‚
â”‚  â”‚ â€¢ ç½®ä¿¡åº¦è¯„åˆ† (0.0-1.0)                   â”‚          â”‚
â”‚  â”‚ â€¢ æ ‡è®°ä½ç½®ä¿¡åº¦åŒºåŸŸ                        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                          â”‚
â”‚  è¾“å‡º: {                                                â”‚
â”‚    chapters: [...],                                     â”‚
â”‚    uncertain_regions: [ä½ç½®ä¿¡åº¦ç« èŠ‚],                   â”‚
â”‚    confidence: 0.85                                     â”‚
â”‚  }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
           â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
           â”‚ ç½®ä¿¡åº¦æ£€æŸ¥ â”‚
           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
     é«˜ç½®ä¿¡            ä½ç½®ä¿¡
    (>0.7)           (<0.7)
        â”‚                 â”‚
        â”‚                 â–¼
        â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    â”‚   ç¬¬äºŒé˜¶æ®µ: LLMè¾…åŠ©å†³ç­– (æ™ºèƒ½)       â”‚
        â”‚    â”‚                                      â”‚
        â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚    â”‚  â”‚ â€¢ ä¸Šä¸‹æ–‡ç†è§£                â”‚    â”‚
        â”‚    â”‚  â”‚ â€¢ è¯­ä¹‰åˆ†æ                  â”‚    â”‚
        â”‚    â”‚  â”‚ â€¢ ç« èŠ‚è¾¹ç•Œåˆ¤æ–­              â”‚    â”‚
        â”‚    â”‚  â”‚ â€¢ æ¶ˆæ­§å¤„ç†                  â”‚    â”‚
        â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”‚    â”‚                                      â”‚
        â”‚    â”‚  ä½¿ç”¨æŠ€æœ¯:                          â”‚
        â”‚    â”‚  - Prompt Caching (é™ä½æˆæœ¬)        â”‚
        â”‚    â”‚  - æ‰¹é‡å¤„ç†                         â”‚
        â”‚    â”‚  - ç»“æ„åŒ–è¾“å‡º (JSON)                â”‚
        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç¬¬ä¸‰é˜¶æ®µ: ç»“æœèåˆä¸ä¼˜åŒ–                        â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ â€¢ åˆå¹¶è§„åˆ™å’ŒLLMç»“æœ                       â”‚          â”‚
â”‚  â”‚ â€¢ ä¸€è‡´æ€§æ£€æŸ¥                              â”‚          â”‚
â”‚  â”‚ â€¢ å†²çªè§£å†³                                â”‚          â”‚
â”‚  â”‚ â€¢ æœ€ç»ˆéªŒè¯                                â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¾“å‡º: ç»“æ„åŒ–ç« èŠ‚                        â”‚
â”‚  {                                                       â”‚
â”‚    volumes: [...],                                      â”‚
â”‚    quality_score: 0.95,                                 â”‚
â”‚    llm_calls: 3                                         â”‚
â”‚  }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å†³ç­–æµç¨‹å›¾

```
å¼€å§‹è§£æ
    â”‚
    â–¼
è§„åˆ™åŒ¹é…
    â”‚
    â”œâ”€â”€â–º åŒ¹é…æˆåŠŸ â”€â”€â–º è®¡ç®—ç½®ä¿¡åº¦
    â”‚                    â”‚
    â”‚                    â”œâ”€â”€â–º é«˜ç½®ä¿¡åº¦(>0.7) â”€â”€â–º æ¥å—
    â”‚                    â”‚
    â”‚                    â””â”€â”€â–º ä½ç½®ä¿¡åº¦(<0.7) â”€â”€â”
    â”‚                                          â”‚
    â””â”€â”€â–º åŒ¹é…å¤±è´¥/æ¨¡ç³Š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                              â”‚
                                              â–¼
                                        éœ€è¦LLMä»‹å…¥
                                              â”‚
                                              â–¼
                                      LLMåˆ†æä¸Šä¸‹æ–‡
                                              â”‚
                                              â”œâ”€â”€â–º ç¡®è®¤ä¸ºç« èŠ‚ â”€â”€â–º æ¥å—
                                              â”‚
                                              â””â”€â”€â–º æ‹’ç»/ä¿®æ­£ â”€â”€â–º è¿‡æ»¤/ä¿®æ­£
                                                      â”‚
                                                      â–¼
                                                  æœ€ç»ˆè¾“å‡º
```

---

## æ ¸å¿ƒç»„ä»¶

### 1. ç½®ä¿¡åº¦è¯„åˆ†ç³»ç»Ÿ

#### è¯„åˆ†æ¨¡å‹

```python
class ConfidenceScorer:
    """ç« èŠ‚ç½®ä¿¡åº¦è¯„åˆ†å™¨"""

    def score_chapter(self, match, content, context):
        """
        ç»¼åˆè¯„åˆ†,è¿”å›0-1ä¹‹é—´çš„ç½®ä¿¡åº¦

        è¯„åˆ†ç»´åº¦:
        1. æ¨¡å¼åŒ¹é…å¼ºåº¦ (40%)
        2. ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ (30%)
        3. ç« èŠ‚é•¿åº¦åˆç†æ€§ (20%)
        4. ä¸å…¶ä»–ç« èŠ‚çš„ä¸€è‡´æ€§ (10%)
        """
        score = 1.0

        # å› ç´ 1: æ¨¡å¼åŒ¹é…å¼ºåº¦ (40%)
        pattern_score = self._pattern_strength(match)
        score *= (0.4 + 0.6 * pattern_score)

        # å› ç´ 2: ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ (30%)
        context_score = self._context_consistency(match, content)
        score *= (0.3 + 0.7 * context_score)

        # å› ç´ 3: ç« èŠ‚é•¿åº¦åˆç†æ€§ (20%)
        length_score = self._length_reasonableness(match, content)
        score *= (0.2 + 0.8 * length_score)

        # å› ç´ 4: ä¸å…¶ä»–ç« èŠ‚çš„ä¸€è‡´æ€§ (10%)
        consistency_score = self._cross_chapter_consistency(match, context)
        score *= (0.1 + 0.9 * consistency_score)

        return score
```

#### è¯„åˆ†ç»†åˆ™

| è¯„åˆ†å› ç´  | é«˜åˆ†æ¡ä»¶ | ä½åˆ†æ¡ä»¶ |
|---------|---------|---------|
| **æ¨¡å¼åŒ¹é…å¼ºåº¦** | "ç¬¬ä¸€ç«  æ ‡é¢˜" (1.0) | æ¨¡ç³ŠåŒ¹é… (0.5) |
| **ä¸Šä¸‹æ–‡ä¸€è‡´æ€§** | ç‹¬å ä¸€è¡Œ+å‰åç©ºè¡Œ (1.0) | å¥å­ä¸­é—´ (0.2) |
| **ç« èŠ‚é•¿åº¦** | 500-50000å­—ç¬¦ (1.0) | <100æˆ–>100000 (0.3) |
| **ç« èŠ‚ä¸€è‡´æ€§** | ä¸å‰åç« èŠ‚é£æ ¼ä¸€è‡´ (1.0) | æ ¼å¼å·®å¼‚å¤§ (0.4) |

#### LLMä»‹å…¥é˜ˆå€¼

```python
def needs_llm_review(self, chapter_info):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦LLMä»‹å…¥"""
    return (
        chapter_info.confidence < 0.7 or              # ç½®ä¿¡åº¦ä½
        chapter_info.is_ambiguous or                  # å­˜åœ¨æ­§ä¹‰
        chapter_info.conflicts_with_neighbors or      # ä¸é‚»è¿‘ç« èŠ‚å†²çª
        chapter_info.unusual_pattern or               # éæ ‡å‡†æ ¼å¼
        chapter_info.inline_reference_suspected       # ç–‘ä¼¼å†…è”å¼•ç”¨
    )
```

---

### 2. LLMè¾…åŠ©æ¨¡å—

#### æ•°æ®ç»“æ„

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class ChapterCandidate:
    """å€™é€‰ç« èŠ‚"""
    text: str                    # ç« èŠ‚æ ‡é¢˜æ–‡æœ¬
    position: int                # åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®
    line_number: int            # è¡Œå·
    confidence: float           # è§„åˆ™è¯„åˆ†çš„ç½®ä¿¡åº¦
    context_before: str         # å‰æ–‡ä¸Šä¸‹æ–‡
    context_after: str          # åæ–‡ä¸Šä¸‹æ–‡
    pattern_type: str           # åŒ¹é…æ¨¡å¼ç±»å‹
    issues: List[str]           # å­˜åœ¨çš„é—®é¢˜åˆ—è¡¨


@dataclass
class LLMDecision:
    """LLMå†³ç­–ç»“æœ"""
    is_chapter: bool            # æ˜¯å¦ä¸ºçœŸå®ç« èŠ‚
    confidence: float           # LLMçš„ç½®ä¿¡åº¦
    reason: str                 # åˆ¤æ–­ç†ç”±
    suggested_title: Optional[str] = None  # å»ºè®®çš„æ ‡é¢˜(å¦‚éœ€ä¿®æ­£)
    suggested_position: Optional[int] = None  # å»ºè®®çš„ä½ç½®
```

#### Promptæ¨¡æ¿è®¾è®¡

##### åœºæ™¯1: ç« èŠ‚è¾¹ç•Œåˆ¤æ–­

```python
CHAPTER_BOUNDARY_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­å“ªäº›æ˜¯çœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ã€‚

ã€æ–‡æ¡£ä¿¡æ¯ã€‘
- æ–‡æ¡£ç±»å‹: {doc_type}
- è¯­è¨€: {language}
- å·²è¯†åˆ«ç« èŠ‚æ•°: {existing_chapter_count}
- å¹³å‡ç« èŠ‚é•¿åº¦: {avg_chapter_length}å­—

ã€å·²ç¡®è®¤ç« èŠ‚ç¤ºä¾‹ã€‘
{existing_chapters_sample}

ã€å¾…åˆ†ææ–‡æœ¬ã€‘
{text_snippet}

ã€å€™é€‰ç« èŠ‚ã€‘
{candidates_list}

è¯·ä»”ç»†åˆ†ææ¯ä¸ªå€™é€‰é¡¹çš„ä¸Šä¸‹æ–‡,åˆ¤æ–­å…¶æ˜¯å¦ä¸ºçœŸæ­£çš„ç« èŠ‚æ ‡é¢˜,è¿˜æ˜¯:
- æ­£æ–‡ä¸­çš„ç« èŠ‚å¼•ç”¨ (å¦‚"åœ¨ç¬¬ä¸‰ç« ä¸­è®¨è®ºè¿‡")
- ç›®å½•æ¡ç›®
- å…¶ä»–éç« èŠ‚æ ‡è®°

è¾“å‡ºJSONæ ¼å¼:
{{
  "decisions": [
    {{
      "candidate": "ç¬¬ä¸€ç«  æ ‡é¢˜",
      "is_chapter": true,
      "confidence": 0.95,
      "reason": "ç‹¬å ä¸€è¡Œ,æ ¼å¼æ ‡å‡†,åè·Ÿæ­£æ–‡å†…å®¹,ä¸å·²è¯†åˆ«ç« èŠ‚é£æ ¼ä¸€è‡´",
      "position": "keep"
    }},
    {{
      "candidate": "åœ¨ç¬¬äºŒç« ä¸­",
      "is_chapter": false,
      "confidence": 0.92,
      "reason": "ä½äºå¥å­ä¸­é—´,å‰æœ‰'åœ¨'å­—,åæœ‰'ä¸­'å­—,æ˜æ˜¾æ˜¯å¼•ç”¨",
      "position": "reject"
    }}
  ],
  "overall_analysis": "å…±è¯†åˆ«2ä¸ªå€™é€‰é¡¹,1ä¸ªæœ‰æ•ˆç« èŠ‚,1ä¸ªå†…è”å¼•ç”¨"
}}

æ³¨æ„äº‹é¡¹:
1. é‡ç‚¹å…³æ³¨å€™é€‰é¡¹åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®å’Œä¸Šä¸‹æ–‡
2. è€ƒè™‘ä¸å·²æœ‰ç« èŠ‚çš„ä¸€è‡´æ€§
3. è¯†åˆ«å¸¸è§çš„å¼•ç”¨æ¨¡å¼
4. ç»™å‡ºæ˜ç¡®çš„åˆ¤æ–­ç†ç”±
"""
```

##### åœºæ™¯2: æ¨¡ç³Šç« èŠ‚æ¶ˆæ­§

```python
DISAMBIGUATION_PROMPT = """
ä»¥ä¸‹æ–‡æœ¬å­˜åœ¨æ­§ä¹‰,éœ€è¦ä½ çš„ä¸“ä¸šåˆ¤æ–­ã€‚

ã€é—®é¢˜æè¿°ã€‘
åœ¨ä»¥ä¸‹æ–‡æœ¬ä¸­,"{ambiguous_text}"å¯èƒ½æ˜¯:
A) ç« èŠ‚æ ‡é¢˜
B) æ­£æ–‡ä¸­çš„è¯è¯­/å¼•ç”¨

ã€æ–‡æœ¬ç‰‡æ®µã€‘
{text_context}

ã€èƒŒæ™¯ä¿¡æ¯ã€‘
- å‰ä¸€ç« èŠ‚: {prev_chapter}
- å‰ä¸€ç« èŠ‚é•¿åº¦: {prev_chapter_length}å­—
- æ–‡æ¡£é£æ ¼: {doc_style}
- è¯­è¨€: {language}

ã€åˆ†æè¦ç‚¹ã€‘
1. è¯¥æ–‡æœ¬æ˜¯å¦ç‹¬å ä¸€è¡Œ?
2. å‰åæ˜¯å¦æœ‰ç©ºè¡Œåˆ†éš”?
3. æ˜¯å¦åœ¨å¥å­çš„è¯­æ³•ç»“æ„ä¸­?
4. æ ¼å¼æ˜¯å¦ç¬¦åˆå…¶ä»–ç« èŠ‚æ ‡é¢˜?
5. å¦‚æœæ˜¯ç« èŠ‚,é•¿åº¦æ˜¯å¦åˆç†?

è¯·ç»™å‡ºä½ çš„åˆ†æ:
{{
  "decision": "chapter" æˆ– "reference" æˆ– "unclear",
  "confidence": 0.0-1.0,
  "analysis": {{
    "line_position": "ç‹¬å ä¸€è¡Œ/å¥å­ä¸­é—´",
    "surrounding_context": "æœ‰ç©ºè¡Œåˆ†éš”/ç´§æ¥ä¸Šæ–‡",
    "format_consistency": "ä¸å…¶ä»–ç« èŠ‚ä¸€è‡´/æ ¼å¼ä¸ç¬¦",
    "length_check": "é•¿åº¦åˆç†/è¿‡çŸ­è¿‡é•¿"
  }},
  "recommendation": "å…·ä½“å»ºè®®",
  "reason": "è¯¦ç»†ç†ç”±"
}}
"""
```

##### åœºæ™¯3: æ— æ ‡è®°æ–‡æœ¬ç»“æ„æ¨æ–­

```python
STRUCTURE_INFERENCE_PROMPT = """
ä»¥ä¸‹æ–‡æœ¬æ²¡æœ‰æ˜æ˜¾çš„ç« èŠ‚æ ‡è®°,è¯·åŸºäºå†…å®¹å’Œè¯­ä¹‰æ¨æ–­ç« èŠ‚ç»“æ„ã€‚

ã€ä»»åŠ¡ã€‘
åˆ†ææ–‡æœ¬çš„ä¸»é¢˜å˜åŒ–,å»ºè®®åˆç†çš„ç« èŠ‚åˆ’åˆ†ã€‚

ã€æ–‡æœ¬æ ·æœ¬ã€‘(å‰{sample_length}å­—)
{content_sample}

ã€åˆ†æç»´åº¦ã€‘
1. ä¸»é¢˜è½¬æ¢ç‚¹: å†…å®¹ä¸»é¢˜å‘ç”Ÿæ˜æ˜¾å˜åŒ–çš„ä½ç½®
2. æ®µè½ç»“æ„: æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„æ®µè½åˆ†ç»„
3. è¯­ä¹‰è¿è´¯æ€§: å“ªäº›æ®µè½åœ¨è¯­ä¹‰ä¸Šç´§å¯†ç›¸å…³
4. è‡ªç„¶åˆ†ç•Œ: æ˜¯å¦æœ‰æ˜æ˜¾çš„åˆ†èŠ‚æ ‡è®°(å¦‚"***", "---")

ã€è¾“å‡ºè¦æ±‚ã€‘
{{
  "suggested_chapters": [
    {{
      "start_position": 0,
      "end_position": 1523,
      "suggested_title": "å¼•è¨€:ç ”ç©¶èƒŒæ™¯",
      "reason": "ä»‹ç»ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº,ä¸»é¢˜ç‹¬ç«‹å®Œæ•´",
      "confidence": 0.85,
      "key_topics": ["èƒŒæ™¯", "åŠ¨æœº", "ç ”ç©¶æ„ä¹‰"]
    }},
    {{
      "start_position": 1524,
      "end_position": 3890,
      "suggested_title": "ç¬¬ä¸€éƒ¨åˆ†:ç†è®ºåŸºç¡€",
      "reason": "è½¬å…¥ç†è®ºé˜è¿°,ä¸å‰æ–‡æ˜æ˜¾ä¸åŒ",
      "confidence": 0.78,
      "key_topics": ["æ¦‚å¿µå®šä¹‰", "ç†è®ºæ¡†æ¶"]
    }}
  ],
  "confidence_level": "high/medium/low",
  "notes": "é¢å¤–è¯´æ˜"
}}

æ³¨æ„:
- ç« èŠ‚æ•°é‡æ§åˆ¶åœ¨åˆç†èŒƒå›´(å»ºè®®5-20ç« )
- æ¯ç« é•¿åº¦å°½é‡å‡è¡¡
- æ ‡é¢˜ç®€æ´æ˜ç¡®
- ç»™å‡ºæ¸…æ™°çš„åˆ’åˆ†ä¾æ®
"""
```

##### åœºæ™¯4: ç‰¹æ®Šæ ¼å¼è¯†åˆ«

```python
SPECIAL_FORMAT_PROMPT = """
è¿™æ˜¯ä¸€æœ¬ç‰¹æ®Šæ ¼å¼çš„ä¹¦ç±,è¯·å¸®åŠ©è¯†åˆ«å…¶ç« èŠ‚ç»“æ„ã€‚

ã€æ–‡æœ¬æ ·æœ¬ã€‘
{text_sample}

ã€è§‚å¯Ÿåˆ°çš„æ¨¡å¼ã€‘
{observed_patterns}

ã€é—®é¢˜ã€‘
1. è¯¥ä¹¦é‡‡ç”¨ä»€ä¹ˆç« èŠ‚æ ‡è®°æ–¹å¼?
2. å¦‚ä½•åŒºåˆ†ç« èŠ‚æ ‡é¢˜å’Œæ­£æ–‡?
3. æ˜¯å¦æœ‰ç‰¹æ®Šçš„ç»“æ„å±‚æ¬¡?

è¯·åˆ†æå¹¶æä¾›:
{{
  "format_type": "ç« å›ä½“/è¯—æ­Œé›†/å‰§æœ¬/è®ºæ–‡/å…¶ä»–",
  "chapter_pattern": "æ­£åˆ™è¡¨è¾¾å¼æˆ–æè¿°",
  "identification_rules": [
    "è§„åˆ™1: ...",
    "è§„åˆ™2: ..."
  ],
  "sample_chapters": [
    {{"title": "...", "position": ...}}
  ],
  "confidence": 0.0-1.0,
  "suggested_regex": "å»ºè®®çš„æ­£åˆ™è¡¨è¾¾å¼"
}}
"""
```

---

### 3. LLMè¾…åŠ©ç±»å®ç°

#### å®Œæ•´å®ç°ä»£ç 

```python
"""
LLM-assisted parser for ambiguous chapter detection
å¤§æ¨¡å‹è¾…åŠ©è§£æå™¨ - ç”¨äºå¤„ç†æ¨¡ç³Šç« èŠ‚è¯†åˆ«
"""
import json
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ChapterCandidate:
    """å€™é€‰ç« èŠ‚æ•°æ®ç»“æ„"""
    text: str
    position: int
    line_number: int
    confidence: float
    context_before: str
    context_after: str
    pattern_type: str
    issues: List[str] = None

    def __post_init__(self):
        if self.issues is None:
            self.issues = []


@dataclass
class LLMDecision:
    """LLMå†³ç­–ç»“æœæ•°æ®ç»“æ„"""
    is_chapter: bool
    confidence: float
    reason: str
    suggested_title: Optional[str] = None
    suggested_position: Optional[int] = None


class LLMParserAssistant:
    """LLMè¾…åŠ©è§£æå™¨"""

    def __init__(self, llm_client, model="claude-3-5-sonnet-20241022"):
        """
        åˆå§‹åŒ–LLMåŠ©æ‰‹

        :param llm_client: Anthropic clientæˆ–å…¶ä»–LLMå®¢æˆ·ç«¯
        :param model: ä½¿ç”¨çš„æ¨¡å‹
        """
        self.client = llm_client
        self.model = model
        self.cache_enabled = True
        self.max_tokens = 4096

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'total_tokens': 0,
            'total_cost': 0.0
        }

    def analyze_chapter_candidates(
        self,
        candidates: List[ChapterCandidate],
        full_content: str,
        existing_chapters: List[Dict],
        doc_context: Dict = None
    ) -> List[LLMDecision]:
        """
        åˆ†æå€™é€‰ç« èŠ‚,åˆ¤æ–­æ˜¯å¦ä¸ºçœŸå®ç« èŠ‚

        :param candidates: å€™é€‰ç« èŠ‚åˆ—è¡¨
        :param full_content: å®Œæ•´æ–‡æœ¬å†…å®¹
        :param existing_chapters: å·²ç¡®è®¤çš„ç« èŠ‚ä¿¡æ¯
        :param doc_context: æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯
        :return: å†³ç­–ç»“æœåˆ—è¡¨
        """
        if not candidates:
            return []

        logger.info(f"LLM analyzing {len(candidates)} candidates...")

        # æ„å»ºprompt
        prompt = self._build_chapter_analysis_prompt(
            candidates,
            full_content,
            existing_chapters,
            doc_context
        )

        # è°ƒç”¨LLM
        response = self._call_llm(prompt, use_cache=True)

        # è§£æå“åº”
        decisions = self._parse_llm_response(response)

        # æ›´æ–°ç»Ÿè®¡
        confirmed = sum(1 for d in decisions if d.is_chapter)
        logger.info(f"LLM confirmed {confirmed}/{len(candidates)} as chapters")

        return decisions

    def infer_chapter_structure(
        self,
        content: str,
        max_length: int = 10000,
        language: str = 'chinese'
    ) -> List[Dict]:
        """
        å¯¹æ— æ˜æ˜¾ç« èŠ‚æ ‡è®°çš„æ–‡æœ¬,æ¨æ–­ç« èŠ‚ç»“æ„

        :param content: æ–‡æœ¬å†…å®¹
        :param max_length: æœ€å¤§åˆ†æé•¿åº¦
        :param language: æ–‡æ¡£è¯­è¨€
        :return: å»ºè®®çš„ç« èŠ‚ç»“æ„
        """
        logger.info(f"LLM inferring structure for {len(content)} chars...")

        # æˆªå–åˆ†ææ ·æœ¬
        sample = content[:max_length]

        prompt = f"""
ä½ æ˜¯æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚ä»¥ä¸‹æ–‡æœ¬æ²¡æœ‰æ˜æ˜¾ç« èŠ‚æ ‡è®°,è¯·åˆ†æå¹¶å»ºè®®ç« èŠ‚åˆ’åˆ†ã€‚

ã€æ–‡æœ¬æ ·æœ¬ã€‘({len(sample)}å­—ç¬¦)
{sample}

ã€è¯­è¨€ã€‘{language}

ã€ä»»åŠ¡ã€‘
1. è¯†åˆ«å†…å®¹çš„ä¸»é¢˜å˜åŒ–ç‚¹
2. å»ºè®®ç« èŠ‚åˆ’åˆ†ä½ç½®
3. ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆæ ‡é¢˜

è¾“å‡ºJSONæ ¼å¼:
{{
  "suggested_chapters": [
    {{
      "start_char": 0,
      "end_char": 500,
      "title": "å»ºè®®æ ‡é¢˜",
      "reason": "åˆ’åˆ†ä¾æ®",
      "confidence": 0.85
    }}
  ],
  "format_analysis": "æ ¼å¼ç‰¹ç‚¹åˆ†æ",
  "confidence": 0.8
}}
"""

        response = self._call_llm(prompt, max_tokens=2048)
        result = self._parse_structure_response(response)

        logger.info(f"LLM suggested {len(result)} chapters")
        return result

    def disambiguate_reference(
        self,
        text_snippet: str,
        candidate: str,
        context: Dict
    ) -> Dict:
        """
        æ¶ˆæ­§:åˆ¤æ–­æ˜¯ç« èŠ‚æ ‡é¢˜è¿˜æ˜¯æ­£æ–‡å¼•ç”¨

        :param text_snippet: åŒ…å«å€™é€‰çš„æ–‡æœ¬ç‰‡æ®µ
        :param candidate: å€™é€‰ç« èŠ‚æ–‡æœ¬
        :param context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        :return: å†³ç­–å­—å…¸
        """
        logger.debug(f"LLM disambiguating: {candidate}")

        prompt = f"""
åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬ä¸­çš„"{candidate}"æ˜¯ç« èŠ‚æ ‡é¢˜è¿˜æ˜¯æ­£æ–‡ä¸­çš„å¼•ç”¨?

ã€æ–‡æœ¬ç‰‡æ®µã€‘
{text_snippet}

ã€ä¸Šä¸‹æ–‡ã€‘
- å‰ä¸€ç« èŠ‚: {context.get('prev_chapter', 'N/A')}
- æ–‡æ¡£ç±»å‹: {context.get('doc_type', 'æœªçŸ¥')}
- è¯­è¨€: {context.get('language', 'æœªçŸ¥')}

åˆ†æè¦ç‚¹:
1. ä½ç½®: ç‹¬å ä¸€è¡Œè¿˜æ˜¯å¥å­ä¸­é—´?
2. è¯­æ³•: æ˜¯å¦åœ¨å¥å­ç»“æ„ä¸­?
3. æ ¼å¼: æ˜¯å¦ç¬¦åˆç« èŠ‚æ ‡é¢˜æ ¼å¼?

å›ç­”æ ¼å¼:
{{
  "type": "chapter" æˆ– "reference",
  "confidence": 0.0-1.0,
  "reason": "åˆ¤æ–­ç†ç”±"
}}
"""

        response = self._call_llm(prompt, max_tokens=256)
        result = json.loads(response)

        logger.debug(f"Decision: {result['type']} (confidence: {result['confidence']})")
        return result

    def identify_special_format(
        self,
        content_sample: str,
        observed_patterns: List[str]
    ) -> Dict:
        """
        è¯†åˆ«ç‰¹æ®Šæ ¼å¼ä¹¦ç±çš„ç« èŠ‚æ¨¡å¼

        :param content_sample: æ–‡æœ¬æ ·æœ¬
        :param observed_patterns: è§‚å¯Ÿåˆ°çš„æ¨¡å¼
        :return: æ ¼å¼è¯†åˆ«ç»“æœ
        """
        logger.info("LLM identifying special format...")

        patterns_text = "\n".join(f"- {p}" for p in observed_patterns)

        prompt = f"""
è¿™æ˜¯ä¸€æœ¬ç‰¹æ®Šæ ¼å¼çš„ä¹¦ç±,è¯·å¸®åŠ©è¯†åˆ«å…¶ç« èŠ‚ç»“æ„ã€‚

ã€æ–‡æœ¬æ ·æœ¬ã€‘
{content_sample[:2000]}

ã€è§‚å¯Ÿåˆ°çš„æ¨¡å¼ã€‘
{patterns_text}

è¯·åˆ†æ:
1. è¯¥ä¹¦é‡‡ç”¨ä»€ä¹ˆç« èŠ‚æ ‡è®°æ–¹å¼?
2. å¦‚ä½•è¯†åˆ«ç« èŠ‚è¾¹ç•Œ?
3. å»ºè®®çš„æ­£åˆ™è¡¨è¾¾å¼

è¾“å‡ºJSON:
{{
  "format_type": "æ ¼å¼ç±»å‹",
  "chapter_pattern": "æ¨¡å¼æè¿°",
  "identification_rules": ["è§„åˆ™1", "è§„åˆ™2"],
  "sample_chapters": [{{"title": "...", "position": 0}}],
  "confidence": 0.8,
  "suggested_regex": "æ­£åˆ™è¡¨è¾¾å¼"
}}
"""

        response = self._call_llm(prompt, max_tokens=1024)
        result = json.loads(response)

        logger.info(f"Identified format: {result.get('format_type', 'unknown')}")
        return result

    def _build_chapter_analysis_prompt(
        self,
        candidates: List[ChapterCandidate],
        full_content: str,
        existing_chapters: List[Dict],
        doc_context: Dict = None
    ) -> str:
        """æ„å»ºç« èŠ‚åˆ†æprompt"""

        doc_context = doc_context or {}

        # è®¡ç®—å¹³å‡ç« èŠ‚é•¿åº¦
        if existing_chapters:
            avg_length = sum(ch.get('length', 0) for ch in existing_chapters) / len(existing_chapters)
        else:
            avg_length = 0

        # æ ¼å¼åŒ–å€™é€‰é¡¹
        candidates_text = []
        for i, c in enumerate(candidates, 1):
            issues_text = f" [é—®é¢˜: {', '.join(c.issues)}]" if c.issues else ""
            candidates_text.append(
                f"{i}. \"{c.text}\" (ç¬¬{c.line_number}è¡Œ, "
                f"ç½®ä¿¡åº¦:{c.confidence:.2f}, ç±»å‹:{c.pattern_type}){issues_text}"
            )

        # æå–æ¯ä¸ªå€™é€‰çš„ä¸Šä¸‹æ–‡
        contexts = []
        for i, c in enumerate(candidates, 1):
            context = f"""
ã€å€™é€‰{i}ä¸Šä¸‹æ–‡ã€‘
å‰æ–‡: ...{c.context_before}
>>> {c.text} <<<
åæ–‡: {c.context_after}...
"""
            contexts.append(context)

        # å·²ç¡®è®¤ç« èŠ‚ç¤ºä¾‹
        chapter_examples = []
        for ch in existing_chapters[:5]:
            chapter_examples.append(f"- {ch.get('title', 'Unknown')}")

        prompt = f"""
ä½ æ˜¯æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹å€™é€‰é¡¹æ˜¯å¦ä¸ºçœŸæ­£çš„ç« èŠ‚æ ‡é¢˜ã€‚

ã€æ–‡æ¡£ä¿¡æ¯ã€‘
- æ–‡æ¡£ç±»å‹: {doc_context.get('doc_type', 'æœªçŸ¥')}
- è¯­è¨€: {doc_context.get('language', 'æœªçŸ¥')}
- å·²è¯†åˆ«ç« èŠ‚æ•°: {len(existing_chapters)}
- å¹³å‡ç« èŠ‚é•¿åº¦: {avg_length:.0f}å­—

ã€å·²ç¡®è®¤ç« èŠ‚ç¤ºä¾‹ã€‘
{chr(10).join(chapter_examples) if chapter_examples else 'æš‚æ— '}

ã€å¾…åˆ¤æ–­å€™é€‰é¡¹ã€‘
{chr(10).join(candidates_text)}

{chr(10).join(contexts)}

ã€åˆ¤æ–­æ ‡å‡†ã€‘
1. âœ“ ç‹¬å ä¸€è¡Œ
2. âœ“ å‰åæœ‰é€‚å½“åˆ†éš”
3. âœ“ ä¸åœ¨å¥å­è¯­æ³•ç»“æ„ä¸­
4. âœ“ æ ¼å¼ä¸å·²è¯†åˆ«ç« èŠ‚ä¸€è‡´
5. âœ— ä½äºå¥å­ä¸­é—´
6. âœ— å‰æœ‰"åœ¨/å¦‚/è§"ç­‰å¼•ç”¨è¯
7. âœ— åæœ‰"ä¸­/é‡Œ/ç»“æŸæ—¶"ç­‰è¿æ¥è¯

è¯·ä¸ºæ¯ä¸ªå€™é€‰ç»™å‡ºåˆ¤æ–­,JSONæ ¼å¼:
{{
  "decisions": [
    {{
      "index": 1,
      "is_chapter": true/false,
      "confidence": 0.0-1.0,
      "reason": "è¯¦ç»†ç†ç”±",
      "action": "accept/reject/modify",
      "suggested_title": "å¦‚éœ€ä¿®æ”¹çš„æ ‡é¢˜"
    }}
  ],
  "overall_analysis": "æ•´ä½“åˆ†æ"
}}
"""
        return prompt

    def _call_llm(
        self,
        prompt: str,
        use_cache: bool = False,
        max_tokens: int = None
    ) -> str:
        """
        è°ƒç”¨LLM API

        :param prompt: æç¤ºè¯
        :param use_cache: æ˜¯å¦ä½¿ç”¨prompt caching
        :param max_tokens: æœ€å¤§tokenæ•°
        :return: LLMå“åº”æ–‡æœ¬
        """
        try:
            self.stats['total_calls'] += 1

            messages = [{"role": "user", "content": prompt}]

            # Anthropic APIè°ƒç”¨
            if hasattr(self.client, 'messages'):
                system_message = {
                    "type": "text",
                    "text": "ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æåŠ©æ‰‹,æ“…é•¿è¯†åˆ«ç« èŠ‚å’Œç›®å½•ç»“æ„ã€‚"
                }

                # å¯ç”¨prompt caching
                if use_cache and self.cache_enabled:
                    system_message["cache_control"] = {"type": "ephemeral"}

                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=max_tokens or self.max_tokens,
                    messages=messages,
                    system=[system_message]
                )

                # æ›´æ–°ç»Ÿè®¡
                usage = response.usage
                self.stats['total_tokens'] += usage.input_tokens + usage.output_tokens

                # ä¼°ç®—æˆæœ¬ (Claude 3.5 Sonnetä»·æ ¼)
                input_cost = usage.input_tokens * 0.003 / 1000
                output_cost = usage.output_tokens * 0.015 / 1000
                self.stats['total_cost'] += input_cost + output_cost

                return response.content[0].text

            # å…¶ä»–LLMå®¢æˆ·ç«¯
            else:
                response = self.client.complete(prompt)
                return response

        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise

    def _parse_llm_response(self, response: str) -> List[LLMDecision]:
        """è§£æLLM JSONå“åº”"""
        try:
            # æå–JSON (å¤„ç†markdownä»£ç å—)
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                response = response[json_start:json_end].strip()
            elif "```" in response:
                json_start = response.find("```") + 3
                json_end = response.find("```", json_start)
                response = response[json_start:json_end].strip()

            data = json.loads(response)
            decisions = []

            for item in data.get('decisions', []):
                decisions.append(LLMDecision(
                    is_chapter=item.get('is_chapter', False),
                    confidence=item.get('confidence', 0.5),
                    reason=item.get('reason', ''),
                    suggested_title=item.get('suggested_title'),
                    suggested_position=item.get('suggested_position')
                ))

            return decisions

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {e}")
            logger.debug(f"Response was: {response}")
            return []

    def _parse_structure_response(self, response: str) -> List[Dict]:
        """è§£æç»“æ„æ¨æ–­å“åº”"""
        try:
            # åŒæ ·å¤„ç†markdown
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                response = response[json_start:json_end].strip()

            data = json.loads(response)
            return data.get('suggested_chapters', [])

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse structure response: {e}")
            return []

    def get_stats(self) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return self.stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.stats = {
            'total_calls': 0,
            'cache_hits': 0,
            'total_tokens': 0,
            'total_cost': 0.0
        }
```

---

### 4. æ··åˆè§£æå™¨å®ç°

```python
class HybridParser:
    """æ··åˆè§£æå™¨: è§„åˆ™ + LLM"""

    def __init__(
        self,
        llm_client=None,
        config: ParserConfig = None
    ):
        """
        åˆå§‹åŒ–æ··åˆè§£æå™¨

        :param llm_client: LLMå®¢æˆ·ç«¯(å¯é€‰)
        :param config: è§£æå™¨é…ç½®
        """
        self.config = config or DEFAULT_CONFIG
        self.rule_parser = RuleBasedParserWithConfidence()
        self.llm_assistant = LLMParserAssistant(llm_client) if llm_client else None

    def parse(self, content: str) -> List[Volume]:
        """
        æ··åˆè§£ææµç¨‹

        :param content: æ–‡æœ¬å†…å®¹
        :return: å·åˆ—è¡¨
        """
        # é˜¶æ®µ1: è§„åˆ™è§£æ + ç½®ä¿¡åº¦è¯„åˆ†
        logger.info("Phase 1: Rule-based parsing...")
        rule_result = self.rule_parser.parse_with_confidence(content)

        # å¦‚æœæ•´ä½“ç½®ä¿¡åº¦é«˜,ç›´æ¥è¿”å›
        if rule_result['overall_confidence'] > 0.85:
            logger.info(f"High confidence ({rule_result['overall_confidence']:.2f}), "
                       f"skipping LLM assistance")
            return rule_result['volumes']

        # é˜¶æ®µ2: è¯†åˆ«éœ€è¦LLMçš„åŒºåŸŸ
        uncertain_regions = rule_result.get('uncertain_regions', [])

        if uncertain_regions and self.llm_assistant:
            logger.info(f"Phase 2: LLM assistance for {len(uncertain_regions)} uncertain regions...")

            # è½¬æ¢ä¸ºå€™é€‰æ ¼å¼
            candidates = self._convert_to_candidates(uncertain_regions, content)

            # LLMåˆ†æ
            llm_decisions = self.llm_assistant.analyze_chapter_candidates(
                candidates,
                content,
                rule_result['chapters'],
                {'language': detect_language(content)}
            )

            # é˜¶æ®µ3: èåˆç»“æœ
            logger.info("Phase 3: Merging results...")
            final_volumes = self._merge_results(
                rule_result['volumes'],
                llm_decisions,
                candidates
            )

            # è¾“å‡ºç»Ÿè®¡
            stats = self.llm_assistant.get_stats()
            logger.info(f"LLM Stats: {stats['total_calls']} calls, "
                       f"${stats['total_cost']:.4f} cost")

            return final_volumes

        # æ— éœ€LLMæˆ–æœªæä¾›å®¢æˆ·ç«¯
        return rule_result['volumes']

    def _convert_to_candidates(
        self,
        uncertain_regions: List[Dict],
        content: str
    ) -> List[ChapterCandidate]:
        """è½¬æ¢ä¸ºå€™é€‰æ ¼å¼"""
        candidates = []

        for region in uncertain_regions:
            chapter = region['chapter']
            confidence = region['confidence']

            # æŸ¥æ‰¾åœ¨å†…å®¹ä¸­çš„ä½ç½®
            position = content.find(chapter.title)

            # æå–ä¸Šä¸‹æ–‡
            context_size = 200
            context_before = content[max(0, position-context_size):position]
            context_after = content[position+len(chapter.title):position+len(chapter.title)+context_size]

            # è®¡ç®—è¡Œå·
            line_number = content[:position].count('\n') + 1

            # ç¡®å®šé—®é¢˜
            issues = []
            if confidence < 0.5:
                issues.append("æä½ç½®ä¿¡åº¦")
            elif confidence < 0.7:
                issues.append("ä½ç½®ä¿¡åº¦")

            if "ç¬¬" in chapter.title and ("åœ¨" in context_before[-10:] or "å¦‚" in context_before[-10:]):
                issues.append("ç–‘ä¼¼å¼•ç”¨")

            candidates.append(ChapterCandidate(
                text=chapter.title,
                position=position,
                line_number=line_number,
                confidence=confidence,
                context_before=context_before,
                context_after=context_after,
                pattern_type=region.get('pattern_type', 'standard'),
                issues=issues
            ))

        return candidates

    def _merge_results(
        self,
        rule_volumes: List[Volume],
        llm_decisions: List[LLMDecision],
        candidates: List[ChapterCandidate]
    ) -> List[Volume]:
        """èåˆè§„åˆ™å’ŒLLMç»“æœ"""

        # åˆ›å»ºå†³ç­–æ˜ å°„
        decision_map = {
            candidates[i].text: llm_decisions[i]
            for i in range(len(llm_decisions))
        }

        # å¤„ç†æ¯ä¸ªå·
        new_volumes = []
        for volume in rule_volumes:
            new_chapters = []

            for chapter in volume.chapters:
                decision = decision_map.get(chapter.title)

                if decision:
                    if decision.is_chapter:
                        # LLMç¡®è®¤ä¸ºç« èŠ‚
                        if decision.suggested_title:
                            # ä½¿ç”¨å»ºè®®çš„æ ‡é¢˜
                            new_chapter = Chapter(
                                title=decision.suggested_title,
                                content=chapter.content,
                                sections=chapter.sections
                            )
                            new_chapters.append(new_chapter)
                        else:
                            new_chapters.append(chapter)
                    else:
                        # LLMæ‹’ç»,ä¸æ·»åŠ 
                        logger.info(f"LLM rejected chapter: {chapter.title}")
                else:
                    # æ— LLMå†³ç­–,ä¿ç•™åŸç»“æœ
                    new_chapters.append(chapter)

            if new_chapters:
                new_volumes.append(Volume(
                    title=volume.title,
                    chapters=new_chapters
                ))

        return new_volumes


class RuleBasedParserWithConfidence:
    """å¸¦ç½®ä¿¡åº¦è¯„åˆ†çš„è§„åˆ™è§£æå™¨"""

    def parse_with_confidence(self, content: str) -> Dict:
        """
        è§£æå†…å®¹å¹¶è¿”å›ç½®ä¿¡åº¦

        :return: {
            'volumes': [...],
            'chapters': [...],
            'uncertain_regions': [...],
            'overall_confidence': 0.85
        }
        """
        from .parser import parse_hierarchical_content

        # ä½¿ç”¨ç°æœ‰è§£æå™¨
        volumes = parse_hierarchical_content(content)

        # ä¸ºæ¯ä¸ªç« èŠ‚è®¡ç®—ç½®ä¿¡åº¦
        chapters_with_confidence = []
        uncertain_regions = []

        for volume in volumes:
            for chapter in volume.chapters:
                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self._estimate_confidence(chapter, content)

                chapter_info = {
                    'chapter': chapter,
                    'confidence': confidence,
                    'volume': volume,
                    'length': len(chapter.content) + sum(len(s.content) for s in chapter.sections)
                }

                chapters_with_confidence.append(chapter_info)

                if confidence < 0.7:
                    uncertain_regions.append(chapter_info)

        if chapters_with_confidence:
            overall_confidence = sum(c['confidence'] for c in chapters_with_confidence) / len(chapters_with_confidence)
        else:
            overall_confidence = 0.0

        return {
            'volumes': volumes,
            'chapters': chapters_with_confidence,
            'uncertain_regions': uncertain_regions,
            'overall_confidence': overall_confidence
        }

    def _estimate_confidence(self, chapter, content: str) -> float:
        """ä¼°ç®—ç« èŠ‚ç½®ä¿¡åº¦"""
        score = 0.6  # åŸºç¡€åˆ†

        # å› ç´ 1: æ ‡é¢˜é•¿åº¦
        title_len = len(chapter.title)
        if 5 <= title_len <= 30:
            score += 0.15
        elif title_len < 5 or title_len > 50:
            score -= 0.1

        # å› ç´ 2: å†…å®¹é•¿åº¦
        total_length = len(chapter.content) + sum(len(s.content) for s in chapter.sections)
        if 500 <= total_length <= 50000:
            score += 0.15
        elif total_length < 100:
            score -= 0.2

        # å› ç´ 3: æ ‡é¢˜æ ¼å¼
        if re.match(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ç« ', chapter.title):
            score += 0.1  # æ ‡å‡†æ ¼å¼

        # å› ç´ 4: ä½ç½®æ£€æŸ¥(ç®€åŒ–)
        position = content.find(chapter.title)
        if position > 0:
            before = content[max(0, position-20):position]
            if re.search(r'[åœ¨å¦‚è§]ç¬¬', before):
                score -= 0.3  # ç–‘ä¼¼å¼•ç”¨

        return max(0.0, min(1.0, score))
```

---

## æˆæœ¬ä¼˜åŒ–

### 1. Prompt Caching

ä½¿ç”¨Anthropicçš„Prompt CachingåŠŸèƒ½:

```python
# ç³»ç»Ÿæç¤ºè¯ä¼šè¢«ç¼“å­˜
system_message = {
    "type": "text",
    "text": "ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æåŠ©æ‰‹...",  # è¿™éƒ¨åˆ†ä¼šè¢«ç¼“å­˜
    "cache_control": {"type": "ephemeral"}
}

# é¦–æ¬¡è°ƒç”¨: æ­£å¸¸è®¡è´¹
# åç»­è°ƒç”¨: ç¼“å­˜å‘½ä¸­, æˆæœ¬é™ä½90%
```

**èŠ‚çœ**: 90%çš„è¾“å…¥tokenæˆæœ¬

### 2. æ‰¹é‡å¤„ç†

```python
def batch_analyze_candidates(self, candidates: List[ChapterCandidate]):
    """ä¸€æ¬¡è°ƒç”¨åˆ†æå¤šä¸ªå€™é€‰,è€Œéå¤šæ¬¡è°ƒç”¨"""
    # ä¸€æ¬¡æ€§å‘é€æ‰€æœ‰å€™é€‰
    # è€Œéé€ä¸ªå‘é€
    pass
```

**èŠ‚çœ**: å‡å°‘APIè°ƒç”¨æ¬¡æ•°

### 3. æ™ºèƒ½è§¦å‘

```python
# ä»…å½“éœ€è¦æ—¶è°ƒç”¨LLM
if overall_confidence > 0.85:
    # è·³è¿‡LLM,ç›´æ¥ä½¿ç”¨è§„åˆ™ç»“æœ
    return rule_result
```

**èŠ‚çœ**: 90%çš„ä¹¦ç±ä¸éœ€è¦LLM

### 4. æˆæœ¬ä¼°ç®—

| åœºæ™¯ | LLMè°ƒç”¨æ¬¡æ•° | æˆæœ¬ä¼°ç®— |
|------|------------|---------|
| æ ‡å‡†ä¹¦ç± | 0 | $0.00 |
| è½»åº¦æ¨¡ç³Š | 1-3æ¬¡ | $0.01-0.03 |
| ä¸­åº¦æ¨¡ç³Š | 3-5æ¬¡ | $0.03-0.05 |
| é‡åº¦æ¨¡ç³Š | 5-10æ¬¡ | $0.05-0.10 |
| ç‰¹æ®Šæ ¼å¼ | 10+æ¬¡ | $0.10+ |

**å¹³å‡æˆæœ¬**: ~$0.02/æœ¬

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: åŸºç¡€é›†æˆ

```python
from anthropic import Anthropic
from tasks.txt_to_epub_core.parser_config import ParserConfig
from tasks.txt_to_epub_core.llm_parser_assistant import HybridParser

# åˆå§‹åŒ–
client = Anthropic(api_key="your-api-key")

config = ParserConfig(
    enable_llm_assistance=True,
    llm_confidence_threshold=0.7
)

# åˆ›å»ºæ··åˆè§£æå™¨
parser = HybridParser(llm_client=client, config=config)

# è§£ææ–‡æœ¬
with open('book.txt', 'r') as f:
    content = f.read()

volumes = parser.parse(content)

print(f"è§£æå®Œæˆ: {len(volumes)} å·")
for volume in volumes:
    print(f"- {volume.title or 'ä¸»ä½“'}: {len(volume.chapters)} ç« ")
```

### ç¤ºä¾‹2: ç‰¹æ®Šæ ¼å¼è¯†åˆ«

```python
# è¯†åˆ«ç‰¹æ®Šæ ¼å¼
assistant = LLMParserAssistant(client)

# æå–æ ·æœ¬
sample = content[:2000]
observed_patterns = ["* * *", "---", "Chapter:", "PART"]

# LLMè¯†åˆ«æ ¼å¼
format_info = assistant.identify_special_format(sample, observed_patterns)

print(f"æ ¼å¼ç±»å‹: {format_info['format_type']}")
print(f"å»ºè®®æ­£åˆ™: {format_info['suggested_regex']}")

# ä½¿ç”¨è¯†åˆ«ç»“æœæ›´æ–°é…ç½®
config.custom_chapter_patterns.append(format_info['suggested_regex'])
```

### ç¤ºä¾‹3: æ¶ˆæ­§å¤„ç†

```python
# å¤„ç†æ¨¡ç³Šç« èŠ‚
ambiguous_text = "åœ¨ç¬¬ä¸‰ç« ä¸­æˆ‘ä»¬è®¨è®ºäº†è¿™ä¸ªé—®é¢˜"

decision = assistant.disambiguate_reference(
    text_snippet=ambiguous_text,
    candidate="ç¬¬ä¸‰ç« ",
    context={'prev_chapter': 'ç¬¬äºŒç«  èƒŒæ™¯', 'language': 'chinese'}
)

if decision['type'] == 'reference':
    print(f"è¿™æ˜¯å¼•ç”¨,ä¸æ˜¯ç« èŠ‚: {decision['reason']}")
```

### ç¤ºä¾‹4: ç»“æ„æ¨æ–­

```python
# å¯¹æ— ç« èŠ‚æ ‡è®°çš„æ–‡æœ¬
result = assistant.infer_chapter_structure(
    content=content,
    max_length=10000,
    language='chinese'
)

for chapter in result:
    print(f"å»ºè®®ç« èŠ‚: {chapter['title']}")
    print(f"  ä½ç½®: {chapter['start_char']}-{chapter['end_char']}")
    print(f"  ç†ç”±: {chapter['reason']}")
    print(f"  ç½®ä¿¡åº¦: {chapter['confidence']}")
```

### ç¤ºä¾‹5: å®Œæ•´è½¬æ¢æµç¨‹

```python
from tasks.txt_to_epub_core.core import txt_to_epub

# å®Œæ•´è½¬æ¢(å¸¦LLMè¾…åŠ©)
result = txt_to_epub(
    txt_file='novel.txt',
    epub_file='novel.epub',
    title='æˆ‘çš„å°è¯´',
    author='ä½œè€…å',
    config=config,  # å¯ç”¨LLMçš„é…ç½®
    llm_client=client,  # ä¼ å…¥LLMå®¢æˆ·ç«¯
    show_progress=True
)

# æŸ¥çœ‹ç»“æœ
print(f"è½¬æ¢æˆåŠŸ: {result['success']}")
print(f"ç« èŠ‚æ•°: {result['chapters_count']}")
print(f"éªŒè¯é€šè¿‡: {result['validation_passed']}")

# æŸ¥çœ‹LLMç»Ÿè®¡
if 'llm_stats' in result:
    stats = result['llm_stats']
    print(f"LLMè°ƒç”¨: {stats['total_calls']}æ¬¡")
    print(f"æ€»æˆæœ¬: ${stats['total_cost']:.4f}")
```

---

## æ€§èƒ½æŒ‡æ ‡

### é¢„æœŸæ•ˆæœå¯¹æ¯”

| æŒ‡æ ‡ | çº¯è§„åˆ™ | æ··åˆæ¨¡å¼(è§„åˆ™+LLM) |
|------|--------|-------------------|
| **å‡†ç¡®ç‡** | 82% | **95%+** â¬†ï¸ |
| **æ ‡å‡†æ ¼å¼è¯†åˆ«** | 95% | 98% â¬†ï¸ |
| **ç‰¹æ®Šæ ¼å¼è¯†åˆ«** | 40% | **85%** â¬†ï¸ |
| **è¯¯è¯†åˆ«ç‡** | 18% | **3%** â¬‡ï¸ |
| **å¤„ç†é€Ÿåº¦** | 8s/æœ¬ | 9-12s/æœ¬ â¬‡ï¸ |
| **æˆæœ¬** | $0 | **~$0.02/æœ¬** |
| **é€‚åº”æ€§** | ä½ | **é«˜** â¬†ï¸ |

### å„åœºæ™¯è¡¨ç°

| ä¹¦ç±ç±»å‹ | è§„åˆ™å‡†ç¡®ç‡ | æ··åˆå‡†ç¡®ç‡ | LLMè°ƒç”¨ | æˆæœ¬ |
|---------|-----------|-----------|---------|------|
| æ ‡å‡†ç½‘æ–‡ | 95% | 98% | 0-1æ¬¡ | $0-0.01 |
| å­¦æœ¯è®ºæ–‡ | 70% | 92% | 2-3æ¬¡ | $0.02 |
| ç‰¹æ®Šæ ¼å¼ | 30% | 88% | 5-8æ¬¡ | $0.05 |
| æ— æ ‡è®°æ–‡æœ¬ | 10% | 75% | 3-5æ¬¡ | $0.03 |
| æ··åˆæ ‡è®° | 60% | 90% | 3-4æ¬¡ | $0.03 |

### é€Ÿåº¦å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            å¤„ç†é€Ÿåº¦å¯¹æ¯”                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ çº¯è§„åˆ™:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 8s                 â”‚
â”‚ æ··åˆ(é«˜ç½®ä¿¡): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 9s              â”‚
â”‚ æ··åˆ(ä¸­ç½®ä¿¡): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10s            â”‚
â”‚ æ··åˆ(ä½ç½®ä¿¡): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12s          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€æ¡†æ¶ (1-2å‘¨)

**ç›®æ ‡**: æ­å»ºLLMé›†æˆåŸºç¡€æ¶æ„

**ä»»åŠ¡**:
- [x] è®¾è®¡ç½®ä¿¡åº¦è¯„åˆ†ç³»ç»Ÿ
- [ ] å®ç° `ConfidenceScorer` ç±»
- [ ] å®ç° `LLMParserAssistant` åŸºç¡€ç±»
- [ ] å®ç° `HybridParser` æ¡†æ¶
- [ ] ç¼–å†™åŸºç¡€promptæ¨¡æ¿
- [ ] å•å…ƒæµ‹è¯•(è¦†ç›–ç‡>80%)

**äº¤ä»˜ç‰©**:
- `confidence_scorer.py`
- `llm_parser_assistant.py`
- `hybrid_parser.py`
- æµ‹è¯•ç”¨ä¾‹

### Phase 2: æ ¸å¿ƒåŠŸèƒ½ (2-3å‘¨)

**ç›®æ ‡**: å®ç°ä¸»è¦åŠŸèƒ½å’Œä¼˜åŒ–

**ä»»åŠ¡**:
- [ ] å®ç°ç« èŠ‚è¾¹ç•Œåˆ¤æ–­
- [ ] å®ç°æ¶ˆæ­§åŠŸèƒ½
- [ ] å®ç°prompt caching
- [ ] å®ç°æ‰¹é‡å¤„ç†
- [ ] æˆæœ¬è¿½è¸ªå’Œç»Ÿè®¡
- [ ] é›†æˆåˆ°ç°æœ‰parser.py

**äº¤ä»˜ç‰©**:
- å®Œæ•´çš„LLMè¾…åŠ©åŠŸèƒ½
- æˆæœ¬ç»Ÿè®¡dashboard
- æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

### Phase 3: é«˜çº§ç‰¹æ€§ (2-3å‘¨)

**ç›®æ ‡**: å¢å¼ºèƒ½åŠ›å’Œç”¨æˆ·ä½“éªŒ

**ä»»åŠ¡**:
- [ ] ç‰¹æ®Šæ ¼å¼è¯†åˆ«
- [ ] ç»“æ„æ¨æ–­åŠŸèƒ½
- [ ] ç”¨æˆ·åé¦ˆæ”¶é›†
- [ ] A/Bæµ‹è¯•æ¡†æ¶
- [ ] é…ç½®ç•Œé¢

**äº¤ä»˜ç‰©**:
- ç‰¹æ®Šæ ¼å¼æ”¯æŒ
- ç”¨æˆ·åé¦ˆç³»ç»Ÿ
- é…ç½®å·¥å…·

### Phase 4: å­¦ä¹ ç³»ç»Ÿ (3-4å‘¨)

**ç›®æ ‡**: æŒç»­æ”¹è¿›å’Œè‡ªé€‚åº”

**ä»»åŠ¡**:
- [ ] è§„åˆ™è‡ªåŠ¨æ›´æ–°
- [ ] æ¡ˆä¾‹åº“æ„å»º
- [ ] æ¨¡å¼å­¦ä¹ 
- [ ] æ€§èƒ½ç›‘æ§
- [ ] è‡ªåŠ¨ä¼˜åŒ–

**äº¤ä»˜ç‰©**:
- è‡ªå­¦ä¹ ç³»ç»Ÿ
- ç›‘æ§dashboard
- ä¼˜åŒ–æŠ¥å‘Š

---

## é…ç½®è¯´æ˜

### parser_config.py æ‰©å±•

```python
@dataclass
class ParserConfig:
    # ... ç°æœ‰é…ç½® ...

    # ========== LLMè¾…åŠ©é…ç½® ==========

    # åŸºç¡€å¼€å…³
    enable_llm_assistance: bool = False
    """æ˜¯å¦å¯ç”¨LLMè¾…åŠ© (é»˜è®¤å…³é—­)"""

    # è§¦å‘æ¡ä»¶
    llm_confidence_threshold: float = 0.7
    """LLMä»‹å…¥çš„ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)"""

    llm_trigger_on_ambiguous: bool = True
    """åœ¨æ¨¡ç³Šæƒ…å†µä¸‹è‡ªåŠ¨è§¦å‘LLM"""

    llm_trigger_on_conflict: bool = True
    """åœ¨ç« èŠ‚å†²çªæ—¶è§¦å‘LLM"""

    # æ¨¡å‹é…ç½®
    llm_model: str = "claude-3-5-sonnet-20241022"
    """ä½¿ç”¨çš„LLMæ¨¡å‹"""

    llm_max_tokens: int = 4096
    """LLMå“åº”çš„æœ€å¤§tokenæ•°"""

    # æˆæœ¬æ§åˆ¶
    llm_max_calls_per_book: int = 10
    """æ¯æœ¬ä¹¦æœ€å¤šLLMè°ƒç”¨æ¬¡æ•°"""

    llm_max_cost_per_book: float = 0.10
    """æ¯æœ¬ä¹¦æœ€é«˜æˆæœ¬ (USD)"""

    llm_cache_enabled: bool = True
    """å¯ç”¨prompt cachingé™ä½æˆæœ¬"""

    # æ‰¹å¤„ç†
    llm_batch_size: int = 5
    """æ‰¹é‡å¤„ç†çš„å€™é€‰ç« èŠ‚æ•°é‡"""

    # ç­–ç•¥é€‰æ‹©
    llm_strategy: str = "hybrid"
    """
    è§£æç­–ç•¥:
    - 'rule_only': ä»…ä½¿ç”¨è§„åˆ™
    - 'hybrid': æ··åˆæ¨¡å¼(æ¨è)
    - 'llm_first': LLMä¼˜å…ˆ
    - 'llm_only': ä»…ä½¿ç”¨LLM
    """

    # åŠŸèƒ½å¼€å…³
    llm_enable_disambiguation: bool = True
    """å¯ç”¨æ¶ˆæ­§åŠŸèƒ½"""

    llm_enable_structure_inference: bool = False
    """å¯ç”¨ç»“æ„æ¨æ–­(å®éªŒæ€§)"""

    llm_enable_format_detection: bool = True
    """å¯ç”¨ç‰¹æ®Šæ ¼å¼æ£€æµ‹"""

    # è°ƒè¯•
    llm_debug_mode: bool = False
    """LLMè°ƒè¯•æ¨¡å¼(è®°å½•æ‰€æœ‰promptå’Œå“åº”)"""

    llm_save_prompts: bool = False
    """ä¿å­˜æ‰€æœ‰promptåˆ°æ–‡ä»¶"""
```

### ä½¿ç”¨é…ç½®ç¤ºä¾‹

```python
# ä¿å®ˆæ¨¡å¼: ä»…å¤„ç†æä½ç½®ä¿¡åº¦
conservative_config = ParserConfig(
    enable_llm_assistance=True,
    llm_confidence_threshold=0.5,  # å¾ˆä½æ‰è§¦å‘
    llm_max_calls_per_book=3,      # æœ€å¤š3æ¬¡
    llm_max_cost_per_book=0.03     # æœ€å¤š$0.03
)

# å¹³è¡¡æ¨¡å¼: æ¨èé…ç½®
balanced_config = ParserConfig(
    enable_llm_assistance=True,
    llm_confidence_threshold=0.7,
    llm_max_calls_per_book=10,
    llm_max_cost_per_book=0.10,
    llm_cache_enabled=True
)

# æ¿€è¿›æ¨¡å¼: æœ€å¤§åŒ–å‡†ç¡®ç‡
aggressive_config = ParserConfig(
    enable_llm_assistance=True,
    llm_confidence_threshold=0.85,  # é«˜é˜ˆå€¼
    llm_max_calls_per_book=20,
    llm_enable_structure_inference=True,
    llm_enable_format_detection=True
)
```

---

## ç›‘æ§å’Œè°ƒè¯•

### ç»Ÿè®¡ä¿¡æ¯

```python
# è·å–LLMä½¿ç”¨ç»Ÿè®¡
stats = assistant.get_stats()

print(f"""
LLMä½¿ç”¨ç»Ÿè®¡:
- æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}
- ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}
- æ€»tokenæ•°: {stats['total_tokens']}
- æ€»æˆæœ¬: ${stats['total_cost']:.4f}
""")
```

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
config = ParserConfig(
    enable_llm_assistance=True,
    llm_debug_mode=True,
    llm_save_prompts=True
)

# è°ƒè¯•ä¿¡æ¯ä¼šè¾“å‡ºåˆ°æ—¥å¿—
# promptä¼šä¿å­˜åˆ° .llm_prompts/ ç›®å½•
```

---

## æœ€ä½³å®è·µ

### 1. ä½•æ—¶å¯ç”¨LLM

âœ… **å»ºè®®å¯ç”¨**:
- å¤„ç†æœªçŸ¥æ¥æºçš„ä¹¦ç±
- ç‰¹æ®Šæ ¼å¼ä¹¦ç±(ç« å›ä½“ã€å‰§æœ¬ç­‰)
- è´¨é‡è¦æ±‚é«˜çš„åœºæ™¯
- ç”¨æˆ·åé¦ˆè¯¯è¯†åˆ«é—®é¢˜

âŒ **å¯ä»¥ä¸å¯ç”¨**:
- æ ‡å‡†ç½‘æ–‡(å‡†ç¡®ç‡å·²ç»å¾ˆé«˜)
- æ‰¹é‡å¤„ç†(æˆæœ¬è€ƒè™‘)
- æ€§èƒ½ä¼˜å…ˆåœºæ™¯
- ç¦»çº¿ç¯å¢ƒ

### 2. æˆæœ¬æ§åˆ¶å»ºè®®

```python
# è®¾ç½®åˆç†çš„é™åˆ¶
config = ParserConfig(
    llm_max_calls_per_book=10,      # é˜²æ­¢å¤±æ§
    llm_max_cost_per_book=0.10,     # æˆæœ¬ä¸Šé™
    llm_cache_enabled=True          # å¿…é¡»å¼€å¯
)
```

### 3. æé«˜å‡†ç¡®ç‡

```python
# æä¾›æ›´å¤šä¸Šä¸‹æ–‡
doc_context = {
    'doc_type': 'å­¦æœ¯è®ºæ–‡',
    'language': 'chinese',
    'known_pattern': 'ç« èŠ‚ç”¨ç½—é©¬æ•°å­—'
}

assistant.analyze_chapter_candidates(
    candidates,
    content,
    existing_chapters,
    doc_context=doc_context  # ä¼ å…¥ä¸Šä¸‹æ–‡
)
```

### 4. é”™è¯¯å¤„ç†

```python
try:
    volumes = parser.parse(content)
except Exception as e:
    logger.error(f"LLM parsing failed: {e}")
    # é™çº§åˆ°çº¯è§„åˆ™è§£æ
    volumes = parse_hierarchical_content(content)
```

---

## FAQ

### Q1: LLMä¼šå¢åŠ å¤šå°‘æˆæœ¬?

**A**: å¹³å‡æ¯æœ¬ä¹¦ $0.01-0.05,å¤§éƒ¨åˆ†ä¹¦ç±æ— éœ€LLM ($0)ã€‚

### Q2: ä¼šä¸ä¼šå˜æ…¢å¾ˆå¤š?

**A**: è½»å¾®å˜æ…¢(1-4ç§’),ä½†å‡†ç¡®ç‡æå‡æ˜¾è‘—(82%â†’95%)ã€‚

### Q3: æ”¯æŒå“ªäº›LLM?

**A**: ä¸»è¦æ”¯æŒAnthropic Claude,å¯æ‰©å±•åˆ°å…¶ä»–æ”¯æŒJSONè¾“å‡ºçš„LLMã€‚

### Q4: å¯ä»¥å®Œå…¨ä¾èµ–LLMå—?

**A**: ä¸å»ºè®®ã€‚è§„åˆ™+LLMæ··åˆæ¨¡å¼æ€§ä»·æ¯”æœ€é«˜ã€‚

### Q5: å¦‚ä½•ä¿æŠ¤APIå¯†é’¥?

**A**: ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†æœåŠ¡,ä¸è¦ç¡¬ç¼–ç ã€‚

### Q6: ç¦»çº¿ç¯å¢ƒèƒ½ç”¨å—?

**A**: LLMåŠŸèƒ½éœ€è¦åœ¨çº¿,ä½†å¯ä»¥é™çº§åˆ°çº¯è§„åˆ™æ¨¡å¼ã€‚

---

## æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

âœ… **å‡†ç¡®ç‡æå‡**: ä»82%æå‡åˆ°95%+
âœ… **æ™ºèƒ½é€‚åº”**: è‡ªåŠ¨å¤„ç†ç‰¹æ®Šæ ¼å¼
âœ… **æˆæœ¬å¯æ§**: å¹³å‡$0.02/æœ¬
âœ… **æ¸è¿›å¢å¼º**: ä¸å½±å“ç°æœ‰åŠŸèƒ½
âœ… **æ˜“äºé›†æˆ**: æœ€å°‘ä»£ç æ”¹åŠ¨

### æŠ€æœ¯äº®ç‚¹

- æ··åˆç­–ç•¥(è§„åˆ™ä¼˜å…ˆ+LLMè¾…åŠ©)
- ç½®ä¿¡åº¦è¯„åˆ†ç³»ç»Ÿ
- Prompt Cachingæˆæœ¬ä¼˜åŒ–
- æ‰¹é‡å¤„ç†å‡å°‘è°ƒç”¨
- ç»“æ„åŒ–è¾“å‡º(JSON)

### é€‚ç”¨åœºæ™¯

é€‚åˆéœ€è¦é«˜å‡†ç¡®ç‡çš„åœºæ™¯,ç‰¹åˆ«æ˜¯:
- ç‰¹æ®Šæ ¼å¼ä¹¦ç±
- è´¨é‡è¦æ±‚é«˜çš„è½¬æ¢
- æœªçŸ¥æ ¼å¼çš„æ–‡æœ¬
- éœ€è¦æ™ºèƒ½æ¶ˆæ­§çš„åœºæ™¯

---

## é™„å½•

### A. Prompt Caching è¯¦è§£

Anthropicçš„Prompt Cachingå¯ä»¥ç¼“å­˜é•¿çš„systemæç¤ºè¯å’Œä¸Šä¸‹æ–‡,å¤§å¹…é™ä½æˆæœ¬:

```
é¦–æ¬¡è°ƒç”¨:
- è¾“å…¥: 5000 tokens ($0.015)
- è¾“å‡º: 500 tokens ($0.0075)
- æ€»æˆæœ¬: $0.0225

åç»­è°ƒç”¨(ç¼“å­˜å‘½ä¸­):
- è¾“å…¥: 5000 tokens cached ($0.0015)  # 90% off
- è¾“å‡º: 500 tokens ($0.0075)
- æ€»æˆæœ¬: $0.009

èŠ‚çœ: 60% â¬‡ï¸
```

### B. é”™è¯¯ç è¯´æ˜

| é”™è¯¯ç  | è¯´æ˜ | å¤„ç†æ–¹å¼ |
|-------|------|---------|
| LLM_001 | APIè°ƒç”¨å¤±è´¥ | é™çº§åˆ°è§„åˆ™è§£æ |
| LLM_002 | JSONè§£æå¤±è´¥ | é‡è¯•æˆ–è·³è¿‡ |
| LLM_003 | è¶…å‡ºæˆæœ¬é™åˆ¶ | åœæ­¢LLMè°ƒç”¨ |
| LLM_004 | è¶…æ—¶ | é‡è¯•æˆ–é™çº§ |

### C. æ€§èƒ½ä¼˜åŒ–æ¸…å•

- [ ] å¯ç”¨prompt caching
- [ ] æ‰¹é‡å¤„ç†å€™é€‰é¡¹
- [ ] è®¾ç½®åˆç†çš„ç½®ä¿¡åº¦é˜ˆå€¼
- [ ] é™åˆ¶æœ€å¤§è°ƒç”¨æ¬¡æ•°
- [ ] ä½¿ç”¨é«˜ç½®ä¿¡åº¦è·³è¿‡æœºåˆ¶
- [ ] ç¼“å­˜LLMå“åº”(æœ¬åœ°)
- [ ] å¼‚æ­¥è°ƒç”¨(å¯é€‰)

---

## æ›´æ–°æ—¥å¿—

**v1.0.0** (2025-12-12)
- åˆå§‹è®¾è®¡æ–‡æ¡£
- å®Œæ•´çš„æ¶æ„å’Œå®ç°æ–¹æ¡ˆ
- è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹

---

## ç›¸å…³èµ„æº

- [Anthropic APIæ–‡æ¡£](https://docs.anthropic.com/)
- [Prompt CachingæŒ‡å—](https://docs.anthropic.com/claude/docs/prompt-caching)
- [OPTIMIZATION_README.md](OPTIMIZATION_README.md) - å·²å®Œæˆçš„ä¼˜åŒ–
- [parser.py](tasks/txt-to-epub-core/parser.py) - ç°æœ‰è§£æå™¨
- [parser_config.py](tasks/txt-to-epub-core/parser_config.py) - é…ç½®ç³»ç»Ÿ

---

**æ–‡æ¡£ç»´æŠ¤**: è¯·åœ¨å®æ–½è¿‡ç¨‹ä¸­åŠæ—¶æ›´æ–°æ­¤æ–‡æ¡£
**åé¦ˆ**: æ¬¢è¿æå‡ºæ”¹è¿›å»ºè®®å’Œé—®é¢˜
